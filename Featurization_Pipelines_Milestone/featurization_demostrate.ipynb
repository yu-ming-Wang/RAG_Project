{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization Pipeline demonstrate\n",
    "### 1. Fetch and decompress data\n",
    "### 2. Clean and Chunk\n",
    "### 3. Generate embedding\n",
    "### 4. Store in qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功提取並解壓縮 26 條數據。\n",
      "ClearML Task: created new task id=587ffc8dea524abc95f62a8cac3fe527\n",
      "2024-12-08 17:39:28,394 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/9e9636ad3f4843dfb1ad2a07ac888206/experiments/587ffc8dea524abc95f62a8cac3fe527/output/log\n",
      "清理後的文本內容（前100字符）: Title: Getting started with ROS2 — Part 1...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Subtitle: Sign up to discover human stories that deepen your understanding of the world....\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Sign up Sign in Sign up Sign in Sharad Maheshwari Follow schmiedeone 88 1 Listen Share Note: This se...\n",
      "切分得到的文本塊數量: 4\n",
      "清理後的文本內容（前100字符）: Subtitle: Manipulation of robot arm with MoveIt2 — Visualizing robot arm in simulation — 1...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Sign up Sign in Sign up Sign in Santosh Balaji Selvaraj Follow Listen Share Please note the source c...\n",
      "切分得到的文本塊數量: 2\n",
      "清理後的文本內容（前100字符）: Title: Tuning the ROS2 Nav2 Stack...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Subtitle: Create an account to read the full story....\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Sign up Sign in Sign up Sign in Member-only story Canyon Lake Robotics Follow Listen Share If you ha...\n",
      "切分得到的文本塊數量: 2\n",
      "清理後的文本內容（前100字符）: Title: Creating a Gazebo Simulation with ROS2 for your own robot...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Subtitle: Section 4.1: URDF vs SDF...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Sign up Sign in Sign up Sign in Tom Schierenebck Follow Creating a Gazebo Simulation with ROS2 for y...\n",
      "切分得到的文本塊數量: 22\n",
      "清理後的文本內容（前100字符）: Title: An Adaptable Approach to Multi-Robot Navigation in ROS2: Utilizing Turtlebot3 and Nav2...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Subtitle: Sign up to discover human stories that deepen your understanding of the world....\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Sign up Sign in Sign up Sign in Arshad Mehmood Follow 7 3 Listen Share In this tutorial, we’ll dive ...\n",
      "切分得到的文本塊數量: 6\n",
      "清理後的文本內容（前100字符）: Title: ROS2 Gazebo World 2D/3D Map Generator...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Subtitle: Sign up to discover human stories that deepen your understanding of the world....\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Sign up Sign in Sign up Sign in Arshad Mehmood Follow 18 Listen Share For realistic navigation utili...\n",
      "切分得到的文本塊數量: 7\n",
      "清理後的文本內容（前100字符）: Title: Exploring NVIDIA Carter Robot in ROS2 Navigation with Isaac Sim and Action Graph...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Subtitle: Sign up to discover human stories that deepen your understanding of the world....\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Sign up Sign in Sign up Sign in Kabilankb Follow 6 Listen Share Introduction: In the realm of roboti...\n",
      "切分得到的文本塊數量: 4\n",
      "清理後的文本內容（前100字符）: Title: Reinforcement Learning Path Planner for 6DOF Robot in ROS2...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Subtitle: Sign up to discover human stories that deepen your understanding of the world....\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Sign up Sign in Sign up Sign in Markus Buchholz Follow Geek Culture 53 Listen Share In this article ...\n",
      "切分得到的文本塊數量: 5\n",
      "清理後的文本內容（前100字符）: Title: Building a ROS 2 Project — Part 1...\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Subtitle: Sign up to discover human stories that deepen your understanding of the world....\n",
      "切分得到的文本塊數量: 1\n",
      "清理後的文本內容（前100字符）: Sign up Sign in Sign up Sign in Sharad Maheshwari Follow 81 1 Listen Share Note: CheckoutPart 0to un...\n",
      "切分得到的文本塊數量: 8\n",
      "ClearML Task: created new task id=44690f40b0ed4b06a91e929fdb0ef39c\n",
      "ClearML results page: https://app.clear.ml/projects/9e9636ad3f4843dfb1ad2a07ac888206/experiments/44690f40b0ed4b06a91e929fdb0ef39c/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonathan\\anaconda3\\envs\\ai\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 77 text chunks using Smollm 360\n",
      "ClearML Task: created new task id=7ae709b13d9e4857b082b035a260b065\n",
      "ClearML results page: https://app.clear.ml/projects/9e9636ad3f4843dfb1ad2a07ac888206/experiments/7ae709b13d9e4857b082b035a260b065/output/log\n",
      "Upserted vector 1/77 into Qdrant\n",
      "Upserted vector 2/77 into Qdrant\n",
      "Upserted vector 3/77 into Qdrant\n",
      "Upserted vector 4/77 into Qdrant\n",
      "Upserted vector 5/77 into Qdrant\n",
      "Upserted vector 6/77 into Qdrant\n",
      "Upserted vector 7/77 into Qdrant\n",
      "Upserted vector 8/77 into Qdrant\n",
      "Upserted vector 9/77 into Qdrant\n",
      "Upserted vector 10/77 into Qdrant\n",
      "Upserted vector 11/77 into Qdrant\n",
      "Upserted vector 12/77 into Qdrant\n",
      "Upserted vector 13/77 into Qdrant\n",
      "Upserted vector 14/77 into Qdrant\n",
      "Upserted vector 15/77 into Qdrant\n",
      "Upserted vector 16/77 into Qdrant\n",
      "Upserted vector 17/77 into Qdrant\n",
      "Upserted vector 18/77 into Qdrant\n",
      "Upserted vector 19/77 into Qdrant\n",
      "Upserted vector 20/77 into Qdrant\n",
      "Upserted vector 21/77 into Qdrant\n",
      "Upserted vector 22/77 into Qdrant\n",
      "Upserted vector 23/77 into Qdrant\n",
      "Upserted vector 24/77 into Qdrant\n",
      "Upserted vector 25/77 into Qdrant\n",
      "Upserted vector 26/77 into Qdrant\n",
      "Upserted vector 27/77 into Qdrant\n",
      "Upserted vector 28/77 into Qdrant\n",
      "Upserted vector 29/77 into Qdrant\n",
      "Upserted vector 30/77 into Qdrant\n",
      "Upserted vector 31/77 into Qdrant\n",
      "Upserted vector 32/77 into Qdrant\n",
      "Upserted vector 33/77 into Qdrant\n",
      "Upserted vector 34/77 into Qdrant\n",
      "Upserted vector 35/77 into Qdrant\n",
      "Upserted vector 36/77 into Qdrant\n",
      "Upserted vector 37/77 into Qdrant\n",
      "Upserted vector 38/77 into Qdrant\n",
      "Upserted vector 39/77 into Qdrant\n",
      "Upserted vector 40/77 into Qdrant\n",
      "Upserted vector 41/77 into Qdrant\n",
      "Upserted vector 42/77 into Qdrant\n",
      "Upserted vector 43/77 into Qdrant\n",
      "Upserted vector 44/77 into Qdrant\n",
      "Upserted vector 45/77 into Qdrant\n",
      "Upserted vector 46/77 into Qdrant\n",
      "Upserted vector 47/77 into Qdrant\n",
      "Upserted vector 48/77 into Qdrant\n",
      "Upserted vector 49/77 into Qdrant\n",
      "Upserted vector 50/77 into Qdrant\n",
      "Upserted vector 51/77 into Qdrant\n",
      "Upserted vector 52/77 into Qdrant\n",
      "Upserted vector 53/77 into Qdrant\n",
      "Upserted vector 54/77 into Qdrant\n",
      "Upserted vector 55/77 into Qdrant\n",
      "Upserted vector 56/77 into Qdrant\n",
      "Upserted vector 57/77 into Qdrant\n",
      "Upserted vector 58/77 into Qdrant\n",
      "Upserted vector 59/77 into Qdrant\n",
      "Upserted vector 60/77 into Qdrant\n",
      "Upserted vector 61/77 into Qdrant\n",
      "Upserted vector 62/77 into Qdrant\n",
      "Upserted vector 63/77 into Qdrant\n",
      "Upserted vector 64/77 into Qdrant\n",
      "Upserted vector 65/77 into Qdrant\n",
      "Upserted vector 66/77 into Qdrant\n",
      "Upserted vector 67/77 into Qdrant\n",
      "Upserted vector 68/77 into Qdrant\n",
      "Upserted vector 69/77 into Qdrant\n",
      "Upserted vector 70/77 into Qdrant\n",
      "Upserted vector 71/77 into Qdrant\n",
      "Upserted vector 72/77 into Qdrant\n",
      "Upserted vector 73/77 into Qdrant\n",
      "Upserted vector 74/77 into Qdrant\n",
      "Upserted vector 75/77 into Qdrant\n",
      "Upserted vector 76/77 into Qdrant\n",
      "Upserted vector 77/77 into Qdrant\n",
      "Stored 77 vectors in Qdrant collection 'demonstrate_embedding'\n",
      "Featurization pipeline successful\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import gzip\n",
    "import base64\n",
    "from typing import Dict, List\n",
    "from clearml import Task\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams\n",
    "from pymongo.errors import ConnectionFailure\n",
    "import uuid\n",
    "\n",
    "# Replace tokenizer with Smollm 135\n",
    "# 步骤1：從 MongoDB 提取和解压數據\n",
    "def decompress_content(compressed_str: str) -> str:\n",
    "    \"\"\"解壓縮base64字符串轉換回的壓縮内容\n",
    "    \"\"\"\n",
    "    try:\n",
    "        compressed_bytes = base64.b64decode(compressed_str)\n",
    "        return gzip.decompress(compressed_bytes).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to decompress content: {e}\")\n",
    "        return \"\"\n",
    "        \n",
    "from gridfs import GridFS\n",
    "\n",
    "def fetch_and_decompress_data(db_name: str = \"llm_twin\", collection_name: str = \"\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    從 MongoDB 中提取數據，包括直接存儲的數據和存儲於 GridFS 的大文件，並進行解壓縮。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "        client.admin.command('ping')  # 確保 MongoDB 服務器可用\n",
    "    except ConnectionFailure:\n",
    "        print(\"無法連接到 MongoDB，請確保 MongoDB 服務正在運行。\")\n",
    "        return []\n",
    "\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    gridfs = GridFS(db)\n",
    "\n",
    "    decompressed_data = []\n",
    "    for doc in collection.find():\n",
    "        if \"gridfs_id\" in doc:  # 如果文檔使用了 GridFS\n",
    "            try:\n",
    "                gridfs_file = gridfs.get(doc[\"gridfs_id\"])  # 獲取 GridFS 文件\n",
    "                compressed_content = gridfs_file.read()  # 讀取壓縮數據\n",
    "                decompressed_text = gzip.decompress(compressed_content).decode('utf-8')  # 解壓縮內容\n",
    "                decompressed_data.append({\n",
    "                    \"text\": decompressed_text,\n",
    "                    \"metadata\": doc.get(\"metadata\", {})  # 保留元數據\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"提取或解壓縮 GridFS 文件失敗，ID: {doc['gridfs_id']}, 錯誤: {e}\")\n",
    "        elif \"content\" in doc and isinstance(doc[\"content\"], dict):  # 直接存儲的數據\n",
    "            for file_name, file_info in doc[\"content\"].items():\n",
    "                if file_info.get(\"type\") == \"compressed\":\n",
    "                    try:\n",
    "                        compressed_content = base64.b64decode(file_info[\"content\"])  # 解碼 base64\n",
    "                        decompressed_text = gzip.decompress(compressed_content).decode('utf-8')  # 解壓縮內容\n",
    "                        decompressed_data.append({\n",
    "                            \"text\": decompressed_text,\n",
    "                            \"metadata\": doc.get(\"source\", {})  # 保留元數據\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"解壓縮內容失敗，文件: {file_name}, 錯誤: {e}\")\n",
    "        else:\n",
    "            print(f\"無法處理文檔: {doc['_id']}, 未知數據格式\")\n",
    "\n",
    "    print(f\"成功提取並解壓縮 {len(decompressed_data)} 條數據。\")\n",
    "    return decompressed_data\n",
    "\n",
    "\n",
    "# 步骤2：清理和切分數據\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"清理文本內容，去掉多餘的空格等无用符號\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 200) -> List[str]:\n",
    "    \"\"\"將文本分割為多個小塊，每個小塊大小為 chunk_size\"\"\"\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def clean_and_chunk_data(fetched_data: List[Dict], chunk_size: int = 200) -> List[Dict]:\n",
    "    \"\"\"清理和切分數據\n",
    "    \"\"\"\n",
    "    task = Task.init(project_name=\"Featurization_Pipeline\", task_name=\"Clean and Chunk Data\")\n",
    "    logger = task.get_logger()\n",
    "\n",
    "    cleaned_chunks = []\n",
    "    for data in fetched_data:\n",
    "        # 清理文本\n",
    "        cleaned_text = clean_text(data[\"text\"])\n",
    "        logger.report_text(f\"清理後的文本內容（前100字符）: {cleaned_text[:100]}...\")\n",
    "        \n",
    "        # 切分文本\n",
    "        chunks = chunk_text(cleaned_text, chunk_size)\n",
    "        logger.report_text(f\"切分得到的文本塊數量: {len(chunks)}\")\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            cleaned_chunks.append({\"text\": chunk, \"metadata\": data[\"metadata\"]})\n",
    "\n",
    "    task.close()\n",
    "    return cleaned_chunks\n",
    "\n",
    "# 步骤3：生成嵌入向量（使用 Smollm 135）\n",
    "huggingface_token = \"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\"\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=huggingface_token)\n",
    "# Add padding token to avoid padding error\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=huggingface_token).half().to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "def generate_embeddings(chunks: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"為每個文本塊生成嵌入，使用 Smollm 135 模型\n",
    "    \"\"\"\n",
    "    embedded_data = []\n",
    "\n",
    "    task = Task.init(project_name=\"Featurization_Pipeline\", task_name=\"Generate Embeddings with Smollm 135\")\n",
    "    logger = task.get_logger()\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # 使用 Smollm 135 生成嵌入\n",
    "        inputs = tokenizer(chunk[\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(torch.device(device))        \n",
    "        with torch.no_grad():                                    \n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.logits.mean(dim=1).squeeze().tolist()\n",
    "           \n",
    "        # 添加到嵌入結果中\n",
    "        embedded_data.append({\n",
    "            \"vector\": embedding,\n",
    "            \"metadata\": chunk[\"metadata\"],\n",
    "            \"text\": chunk[\"text\"]  # 添加原始文本內容\n",
    "        })\n",
    "\n",
    "    logger.report_text(f\"Generated embeddings for {len(chunks)} text chunks using Smollm 360\")\n",
    "    task.close()\n",
    "    return embedded_data\n",
    "\n",
    "def store_in_qdrant(embedded_data: List[Dict], collection_name: str = \"\"):\n",
    "    \"\"\"將嵌入存儲到 Qdrant 向量數據庫中\"\"\"\n",
    "    # Determine vector size from the first embedding\n",
    "    vector_size = len(embedded_data[0][\"vector\"])\n",
    "\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    task = Task.init(project_name=\"Featurization_Pipeline\", task_name=\"Store in Qdrant\")\n",
    "    logger = task.get_logger()\n",
    "\n",
    "    # 如果集合不存在，創建新集合\n",
    "    if not client.collection_exists(collection_name=collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=\"Cosine\")\n",
    "        )\n",
    "\n",
    "    # 將嵌入數據插入到集合中\n",
    "    for idx, data in enumerate(embedded_data):\n",
    "        client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=[\n",
    "                {\n",
    "                    \"id\": str(uuid.uuid4()),  # 使用 UUID 作為唯一標識符\n",
    "                    \"vector\": data[\"vector\"],\n",
    "                    \"payload\": {\n",
    "                        \"text\": data[\"text\"],  # 在 payload 中加入原始文本\n",
    "                        **data[\"metadata\"]  # 保留元數據\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        # 添加日志以追蹤每個向量的存儲\n",
    "        logger.report_text(f\"Upserted vector {idx + 1}/{len(embedded_data)} into Qdrant\")\n",
    "\n",
    "    logger.report_text(f\"Stored {len(embedded_data)} vectors in Qdrant collection '{collection_name}'\")\n",
    "    task.close()\n",
    "\n",
    "\n",
    "# 構建完整的 Pipeline\n",
    "def featurization_pipeline():\n",
    "\n",
    "    # 初始化MongoDB數據庫和集合名稱\n",
    "    db_name = \"llm_twin\"\n",
    "    collection_name = \"Medium\"\n",
    "    qdrant_collection_name = \"demonstrate_embedding\"\n",
    "\n",
    "    # Step 1: Fetch and Decompress Data\n",
    "    decompressed_data = fetch_and_decompress_data(db_name, collection_name)\n",
    "    # Step 2: Clean and Chunk Text\n",
    "    cleaned_chunks = clean_and_chunk_data(decompressed_data)\n",
    "    \n",
    "    # Step 3: Generate Embeddings\n",
    "    embedded_data = generate_embeddings(cleaned_chunks)\n",
    " \n",
    "    # Step 4: Store Embeddings in Qdrant and log to ClearML\n",
    "    store_in_qdrant(embedded_data, qdrant_collection_name)\n",
    "    print(\"Featurization pipeline successful\")\n",
    "\n",
    "\n",
    "\n",
    "featurization_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
