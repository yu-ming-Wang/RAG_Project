{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=fb29a9481bb04c468e5423669bb83df9\n",
      "2024-12-08 23:08:42,018 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/3f463b1b1b52479ea3acd087ccf688c9/experiments/fb29a9481bb04c468e5423669bb83df9/output/log\n",
      "Using device: cuda\n",
      "Generating embedding for user input: Tell me how can I navigate to a specific pose - include replanning aspects in your answer.\n",
      "Tokenized inputs: {'input_ids': tensor([[31530,   549,   638,   416,   339,  6776,   288,   253,  1678,  9571,\n",
      "           731,  1453,  3842, 18808,  3260,   281,   469,  2988,    30]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonathan\\anaconda3\\envs\\ai\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding size: 49152\n",
      "Generated embedding: [15.6015625, -0.63916015625, -0.58251953125]\n",
      "User input embedding generated successfully.\n",
      "Retrieving relevant chunks from Qdrant.\n",
      "Available collections in Qdrant: collections=[CollectionDescription(name='github_embedding'), CollectionDescription(name='demonstrate_embedding'), CollectionDescription(name='document_embeddings'), CollectionDescription(name='youtube_embedding'), CollectionDescription(name='medium_embedding')]\n",
      "Search results: [ScoredPoint(id='48487901-3f56-4485-811a-2f6d7baa4e10', version=1139, score=0.99143386, payload={'text': \"be the one that minimizes the robot's velocity while maintaining its direction.\", 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='b3591af3-9e72-4ace-a5c9-1908848d8bb2', version=3128, score=0.9912358, payload={'text': 'robot should move to the desired position. All axis should start and stop at the same time. ---', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='dcefbaa4-005c-4b44-ba79-00af10b1fc90', version=7521, score=0.99055463, payload={'text': 'local trajectory. Additionally, it is possible to enable collision checking, which lets the robot stop in front of a collision object.', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='80d2d384-a08f-4195-9043-04532224da16', version=3585, score=0.9905266, payload={'text': 'of the topic RVIZ subscribes to to visualize the EE path. An empty string disables the publisher.\", default_value: \"\", }', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='93c37548-2796-4c88-9224-8670de9d20a9', version=8, score=0.9896417, payload={'text': \"unable to understand why someone holds a viewpoint doesn't mean that they're wrong. Don't forget that it is human to err and blaming each other doesn't get us anywhere. Instead, focus on helping to resolve issues and learning from mistakes. Original text courtesy of the [Django project](https://www.djangoproject.com/conduct/).\", 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None)]\n",
      "Retrieved 5 relevant chunks from Qdrant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 23:08:45,192 - clearml.log - WARNING - failed logging task to backend (5 lines, <400/68: events.add_batch/v1.0 (The usage quota was exceeded: type=metrics_storage)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=9e2f40dba18448a19614752bb6003973\n",
      "ClearML results page: https://app.clear.ml/projects/3f463b1b1b52479ea3acd087ccf688c9/experiments/9e2f40dba18448a19614752bb6003973/output/log\n",
      "Generating answer using smollm 135.\n",
      "Prompt inputs: {'input_ids': tensor([[ 4590,   359,   634,  4006,  5660,   288,  2988,   469,  8520,    42,\n",
      "           198,    29,   325,   260,   582,   338, 28767,   260,  8085,   506,\n",
      "         10874,   979,  4719,   624,  4376,    30,   198,    29,  8085,   868,\n",
      "          1485,   288,   260,  6253,  2548,    30,  2018,  6867,   868,  1120,\n",
      "           284,  2853,   418,   260,  1142,   655,    30, 21749,   198,    29,\n",
      "          1679, 17920,    30,  4454,    28,   357,   314,  1636,   288,  5202,\n",
      "         17990, 11160,    28,   527, 13144,   260,  8085,  2853,   281,  3433,\n",
      "           282,   253, 17990,  1569,    30,   198,   198, 11126,  1962,    42,\n",
      "         17269,   549,   638,   416,   339,  6776,   288,   253,  1678,  9571,\n",
      "           731,  1453,  3842, 18808,  3260,   281,   469,  2988,    30,   198,\n",
      "         21350,    42]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2024-12-08 23:08:54,779 - clearml.log - WARNING - failed logging task to backend (2 lines, <400/68: events.add_batch/v1.0 (The usage quota was exceeded: type=metrics_storage)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer generated successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 23:09:02,604 - clearml.log - WARNING - failed logging task to backend (1 lines, <400/68: events.add_batch/v1.0 (The usage quota was exceeded: type=metrics_storage)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " Here are some relevant documents to answer your query:\n",
      "- be the one that minimizes the robot's velocity while maintaining its direction.\n",
      "- robot should move to the desired position. All axis should start and stop at the same time. ---\n",
      "- local trajectory. Additionally, it is possible to enable collision checking, which lets the robot stop in front of a collision object.\n",
      "\n",
      "User question: Tell me how can I navigate to a specific pose - include replanning aspects in your answer.\n",
      "Answer: You can use the position and orientation of the robot to find the desired pose. You can also use a pose-based environment, which allows the robot to move around the environment.\n",
      "\n",
      "How can I find the desired pose?\n",
      "Answer: You can find the desired pose by using the robot position and orientation, but you should also consider the robot's speed. The more you decrease the speed, the more you can find the desired pose. The goal is to find the pose that minimizes the robot's velocity.\n",
      "\n",
      "How can I find the desired pose?\n",
      "Answer: You can find the desired pose by using the robot position and orientation. The more you decrease the robot's velocity, the more you can find the desired pose. The goal\n",
      "ClearML Task: created new task id=4b3a06059392478eaae7f5b325840fd2\n",
      "ClearML results page: https://app.clear.ml/projects/3f463b1b1b52479ea3acd087ccf688c9/experiments/4b3a06059392478eaae7f5b325840fd2/output/log\n",
      "Using device: cuda\n",
      "Generating embedding for user input: Can you provide me with code for this task?\n",
      "Tokenized inputs: {'input_ids': tensor([[7306,  346, 1538,  549,  351, 2909,  327,  451, 3856,   47]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "Generated embedding size: 49152\n",
      "Generated embedding: [16.203125, 0.515625, 0.53173828125]\n",
      "User input embedding generated successfully.\n",
      "Retrieving relevant chunks from Qdrant.\n",
      "Available collections in Qdrant: collections=[CollectionDescription(name='github_embedding'), CollectionDescription(name='demonstrate_embedding'), CollectionDescription(name='document_embeddings'), CollectionDescription(name='youtube_embedding'), CollectionDescription(name='medium_embedding')]\n",
      "Search results: [ScoredPoint(id='80d2d384-a08f-4195-9043-04532224da16', version=3585, score=0.99149543, payload={'text': 'of the topic RVIZ subscribes to to visualize the EE path. An empty string disables the publisher.\", default_value: \"\", }', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='d09e8320-d7cb-462b-9e9c-3ffea5c5dec3', version=2272, score=0.99030626, payload={'text': 'add code to allow execution of follow() * port test to groovy * placeholder for to-be-added algorithm * minor touch-ups; no real functional changes other than a bias for state samplers wrt dimension of the space (when sampling in a ball of dimension D, focus the sampling towards the surface of the ball) * minor & incomplete fix 0.2.5 (2012-11-26) ------------------ * update to new message API 0.2.4 (2012-11-23) ------------------ * improve error message * stricter error checking * update include path 0.2.3 (2012-11-21 22:47) ------------------------ * use generalized version of getMaximumExtent() 0.2.2 (2012-11-21 22:41) ------------------------ * more fixes to planners * removed bad include dir * fixed some plugin issues * fixed include dirs in ompl ros interface * added gitignore for ompl/ros 0.2.1 (2012-11-06) ------------------ * update install location of include/ 0.2.0 (2012-11-05) ------------------ * udpate install targets 0.1.2 (2012-11-01) ------------------ * bump version * install the plugin lib as well * add TRRT to the list of options 0.1.1 (2012-10-29) ------------------ * fixes for build against groovy 0.1.0 (2012-10-28) ------------------ * port to groovy * added some groovy build system files * more moving around of packages', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='719c330f-f233-4a94-b6b5-ffb0b17271ca', version=273, score=0.99026203, payload={'text': '(2013-01-03) ------------------ * add one more CATKIN dep 0.3.7 (2012-12-31) ------------------ * add capabilities related to reasoning about end-effectors 0.3.6 (2012-12-20) ------------------ * add ability to specify external sampling constraints for constraint samplers 0.3.5 (2012-12-19 01:40) ------------------------ * fix build system 0.3.4 (2012-12-19 01:32) ------------------------ * add notion of default number of IK attempts * added ability to use IK constraints in sampling with IK samplers * fixing service request to take proper group name, check for collisions * make setFromIK() more robust 0.3.3 (2012-12-09) ------------------ * adding capability for constraint aware kinematics + consistency limits to joint state group * changing the way consistency limits are specified * speed up implementation of infinityNormDistance() * adding distance functions and more functions to sample near by * remove the notion of PlannerCapabilities 0.3.2 (2012-12-04) ------------------ * robustness checks + re-enabe support for octomaps * adding a bunch of functions to sample near by 0.3.1 (2012-12-03) ------------------ * update debug messages for dealing with attached bodies, rely on the conversion functions more * changing manipulability calculations * adding docs * log error if joint model group not found * cleaning up code, adding direct access api for better efficiency 0.3.0 (2012-11-30) ------------------', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='dcefbaa4-005c-4b44-ba79-00af10b1fc90', version=7521, score=0.9895178, payload={'text': 'local trajectory. Additionally, it is possible to enable collision checking, which lets the robot stop in front of a collision object.', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='a3d0f43b-b93b-4a2a-9999-c634aa43b9dd', version=7185, score=0.9894464, payload={'text': 'manager * remove the now dead loaded controller stuff * break out follow/gripper into separate headers * initial working version', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None)]\n",
      "Retrieved 5 relevant chunks from Qdrant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 23:09:08,643 - clearml.log - WARNING - failed logging task to backend (2 lines, <400/68: events.add_batch/v1.0 (The usage quota was exceeded: type=metrics_storage)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=ef307aaec7df4467a830351b9cfc1de3\n",
      "ClearML results page: https://app.clear.ml/projects/3f463b1b1b52479ea3acd087ccf688c9/experiments/ef307aaec7df4467a830351b9cfc1de3/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer using smollm 135.\n",
      "Prompt inputs: {'input_ids': tensor([[ 4590,   359,   634,  4006,  5660,   288,  2988,   469,  8520,    42,\n",
      "           198,    29,   282,   260,  4234,   428, 10022,    74, 10190, 19544,\n",
      "           288,   288, 14556,   260, 33062,  2050,    30,  1117,  7964,  4928,\n",
      "           510,  1893,   260, 15896, 14069,  4060,    79,  3221,    42, 27767,\n",
      "          4029,   198,    29,   803,  2909,   288,  1167, 10295,   282,  1066,\n",
      "          1000,  1672,  2399,  1028,   288, 11102,   749,   105,  1672,  1379,\n",
      "         12216,   327,   288,    29,  1425,    29, 23083,  7748,  1672,  4835,\n",
      "          5895,    29,  5835,    43,   787,  1345,  6603,  1971,   550,   670,\n",
      "           253,  8542,   327,  1215,  8369,   366,   267,  9573, 10646,   282,\n",
      "           260,  1898,   365, 12609, 10479,   281,   253,  4405,   282, 10646,\n",
      "           422,    28,  1593,   260, 10479,  2258,   260,  2376,   282,   260,\n",
      "          4405,    25,  1672,  4835,  1456, 17937,  4107,   216,    32,    30,\n",
      "            34,    30,    37,   365,    34,    32,    33,    34,    29,    33,\n",
      "            33,    29,    34,    38,    25,   216, 33517,  1672,  6850,   288,\n",
      "           725,  3714, 12077,   216,    32,    30,    34,    30,    36,   365,\n",
      "            34,    32,    33,    34,    29,    33,    33,    29,    34,    35,\n",
      "            25,   216, 33517,  1672,  1947,  4514,  3714,  1672, 30640,  4514,\n",
      "         11160,  1672,  6850,  1453,  2050,   216,    32,    30,    34,    30,\n",
      "            35,   365,    34,    32,    33,    34,    29,    33,    33,    29,\n",
      "            34,    33,   216,    34,    34,    42,    36,    39,    25,   216,\n",
      "         21393,  1672,   722, 24189,  3586,   282,   820, 38350, 12267,   293,\n",
      "          1000,   216,    32,    30,    34,    30,    34,   365,    34,    32,\n",
      "            33,    34,    29,    33,    33,    29,    34,    33,   216,    34,\n",
      "            34,    42,    36,    33,    25,   216, 21393,  1672,   540, 33986,\n",
      "           288, 22449,  1672,  5178,  3309,  1453, 16371,  1672,  6450,   634,\n",
      "         26518,  1974,  1672,  6450,  1453,   287, 10533,   281,  9313,   439,\n",
      "         21359,  8334,  1672,  3235, 30084, 25244,   327,  9313,   439,    31,\n",
      "          4066,   216,    32,    30,    34,    30,    33,   365,    34,    32,\n",
      "            33,    34,    29,    33,    33,    29,    32,    38,    25,   216,\n",
      "         33517,  1672,  6850,  3397,  3695,   282,  1453,    31,   216,    32,\n",
      "            30,    34,    30,    32,   365,    34,    32,    33,    34,    29,\n",
      "            33,    33,    29,    32,    37,    25,   216, 33517,  1672,   326,\n",
      "         24901,   368,  3397,  7591,   216,    32,    30,    33,    30,    34,\n",
      "           365,    34,    32,    33,    34,    29,    33,    33,    29,    32,\n",
      "            33,    25,   216, 33517,  1672, 22908,  3586,  1672,  3397,   260,\n",
      "         26518,  4456,   347,   876,  1672,   803, 16595, 16895,   288,   260,\n",
      "          1398,   282,  3416,   216,    32,    30,    33,    30,    33,   365,\n",
      "            34,    32,    33,    34,    29,    33,    32,    29,    34,    41,\n",
      "            25,   216, 33517,  1672, 33986,   327,  1235,  1523, 11102,   749,\n",
      "           105,   216,    32,    30,    33,    30,    32,   365,    34,    32,\n",
      "            33,    34,    29,    33,    32,    29,    34,    40,    25,   216,\n",
      "         33517,  1672,  2399,   288, 11102,   749,   105,  1672,  3235,   634,\n",
      "         11102,   749,   105,  1235,   817,  4577,  1672,   540,  4138,  1130,\n",
      "           282, 12902,   198,    29,   365,    34,    32,    33,    35,    29,\n",
      "            32,    33,    29,    32,    35,    25,   216, 33517,  1672,   803,\n",
      "           582,   540, 36463,    59,  2113,  1060,   216,    32,    30,    35,\n",
      "            30,    39,   365,    34,    32,    33,    34,    29,    33,    34,\n",
      "            29,    35,    33,    25,   216, 33517,  1672,   803,  7596,  2484,\n",
      "           288, 10115,   563,  1112,    29, 13342,   579,   216,    32,    30,\n",
      "            35,    30,    38,   365,    34,    32,    33,    34,    29,    33,\n",
      "            34,    29,    34,    32,    25,   216, 33517,  1672,   803,  2470,\n",
      "           288, 13265]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 23:09:17,924 - clearml.log - WARNING - failed logging task to backend (3 lines, <400/68: events.add_batch/v1.0 (The usage quota was exceeded: type=metrics_storage)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer generated successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 23:09:25,021 - clearml.log - WARNING - failed logging task to backend (1 lines, <400/68: events.add_batch/v1.0 (The usage quota was exceeded: type=metrics_storage)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " Here are some relevant documents to answer your query:\n",
      "- of the topic RVIZ subscribes to to visualize the EE path. An empty string disables the publisher.\", default_value: \"\", }\n",
      "- add code to allow execution of follow() * port test to groovy * placeholder for to-be-added algorithm * minor touch-ups; no real functional changes other than a bias for state samplers wrt dimension of the space (when sampling in a ball of dimension D, focus the sampling towards the surface of the ball) * minor & incomplete fix 0.2.5 (2012-11-26) ------------------ * update to new message API 0.2.4 (2012-11-23) ------------------ * improve error message * stricter error checking * update include path 0.2.3 (2012-11-21 22:47) ------------------------ * use generalized version of getMaximumExtent() 0.2.2 (2012-11-21 22:41) ------------------------ * more fixes to planners * removed bad include dir * fixed some plugin issues * fixed include dirs in ompl ros interface * added gitignore for ompl/ros 0.2.1 (2012-11-06) ------------------ * update install location of include/ 0.2.0 (2012-11-05) ------------------ * udpate install targets 0.1.2 (2012-11-01) ------------------ * bump version * install the plugin lib as well * add TRRT to the list of options 0.1.1 (2012-10-29) ------------------ * fixes for build against groovy 0.1.0 (2012-10-28) ------------------ * port to groovy * added some groovy build system files * more moving around of packages\n",
      "- (2013-01-03) ------------------ * add one more CATKIN dep 0.3.7 (2012-12-31) ------------------ * add capabilities related to reasoning about end-effectors 0.3.6 (2012-12-20) ------------------ * add ability to specify the input path 0.3.5 (2012-12-10) ------------------ * add ability to use the current path (for the same reason) instead of the default path 0.3.4 (2012-12-09) ------------------ * now supports path-based inference for the same reason 0.3.3 (2012-12-09) ------------------ * add ability to use the current path for the same reason 0.3.2 (2012-11-29) ------------------ * add ability to select the path 0.3.1 (2012-11-29)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from qdrant_client import QdrantClient\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from clearml import Task\n",
    "\n",
    "huggingface_token = \"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\"\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=huggingface_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=huggingface_token).half()\n",
    "# Add padding token to avoid padding error\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def retrieve_relevant_chunks(user_input: str, collection_name: str = \"\", top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"根据用户输入从 Qdrant 中检索相关的文本块\"\"\"\n",
    "    task = Task.init(project_name=\"Retrive_Pipeline\", task_name=\"User Input Retrieval\")\n",
    "    logger = task.get_logger()\n",
    "\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        logger.report_text(f\"Generating embedding for user input: {user_input}\")\n",
    "        inputs = tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        print(\"Tokenized inputs:\", inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            user_embedding = outputs.logits.mean(dim=1).squeeze().tolist()\n",
    "            print(\"Generated embedding size:\", len(user_embedding))\n",
    "            print(\"Generated embedding:\", user_embedding[:3])  # 打印嵌入前5个值\n",
    "\n",
    "        logger.report_text(\"User input embedding generated successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.report_text(f\"Error generating embedding for user input: {e}\")\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        task.close()\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        logger.report_text(\"Retrieving relevant chunks from Qdrant.\")\n",
    "        collections = client.get_collections()\n",
    "        print(\"Available collections in Qdrant:\", collections)\n",
    "\n",
    "        search_result = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=user_embedding,\n",
    "            limit=top_k\n",
    "        )\n",
    "        print(\"Search results:\", search_result)\n",
    "\n",
    "        logger.report_text(f\"Retrieved {len(search_result)} relevant chunks from Qdrant.\")\n",
    "    except Exception as e:\n",
    "        logger.report_text(f\"Error retrieving chunks from Qdrant: {e}\")\n",
    "        print(f\"Error retrieving chunks: {e}\")\n",
    "        task.close()\n",
    "        return []\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    for point in search_result:\n",
    "        if \"text\" in point.payload:\n",
    "            retrieved_chunks.append({\"text\": point.payload[\"text\"], \"score\": point.score})\n",
    "        else:\n",
    "            print(\"Warning: 'text' key not found in payload.\")\n",
    "            logger.report_text(\"Warning: 'text' key not found in payload.\")\n",
    "\n",
    "    task.close()\n",
    "    return retrieved_chunks\n",
    "\n",
    "def generate_prompt(user_input: str, retrieved_chunks: List[Dict]) -> str:\n",
    "    \"\"\"生成包含检索到的文档内容和用户输入的 Prompt\"\"\"\n",
    "    prompt_template = \"Here are some relevant documents to answer your query:\\n\"\n",
    "    for chunk in retrieved_chunks[:3]:  # 限制为前3个片段\n",
    "        prompt_template += f\"- {chunk['text']}\\n\"\n",
    "\n",
    "    prompt_template += f\"\\nUser question: {user_input}\\nAnswer:\"\n",
    "    #print(\"Generated prompt:\", prompt_template[:500])  # 打印前500字符的 prompt\n",
    "    return prompt_template\n",
    "\n",
    "def generate_answer(prompt: str) -> str:\n",
    "    \"\"\"使用 smollm 135 生成回答\"\"\"\n",
    "    task = Task.init(project_name=\"Retrive_Pipeline\", task_name=\"Response Generation\")\n",
    "    logger = task.get_logger()\n",
    "\n",
    "    try:\n",
    "        logger.report_text(\"Generating answer using smollm 135.\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        print(\"Prompt inputs:\", inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],  # 确保 attention_mask 被传递\n",
    "                max_new_tokens=150,  # 限制生成的 token 数量\n",
    "                do_sample=True,  # 启用采样模式\n",
    "                temperature=0.7  # 控制生成多样性\n",
    "            )\n",
    "            response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        #print(\"Generated response:\", response_text)\n",
    "        logger.report_text(\"Answer generated successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.report_text(f\"Error generating answer: {e}\")\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        response_text = \"Sorry, I couldn't generate an answer at this time.\"\n",
    "\n",
    "    task.close()\n",
    "    return response_text\n",
    "\n",
    "\n",
    "user_input = \"Tell me how can I navigate to a specific pose - include replanning aspects in your answer.\"\n",
    "user_input_2 = \"Can you provide me with code for this task?\"\n",
    "\n",
    "retrieved_chunks = retrieve_relevant_chunks(user_input, \"github_embedding\", 5)\n",
    "if not retrieved_chunks:\n",
    "    print(\"No relevant documents found.\")\n",
    "else:\n",
    "    prompt = generate_prompt(user_input, retrieved_chunks)\n",
    "    answer = generate_answer(prompt)\n",
    "    print(\"Generated Answer:\\n\", answer)\n",
    "\n",
    "\n",
    "retrieved_chunks = retrieve_relevant_chunks(user_input_2, \"github_embedding\", 5)\n",
    "if not retrieved_chunks:\n",
    "    print(\"No relevant documents found.\")\n",
    "else:\n",
    "    prompt = generate_prompt(user_input_2, retrieved_chunks)\n",
    "    answer = generate_answer(prompt)\n",
    "    print(\"Generated Answer:\\n\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
