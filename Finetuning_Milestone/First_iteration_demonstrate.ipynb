{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate retrievepipline and first iteration of RAG system\n",
    "\n",
    "### 1. Generate User inputs' embedding\n",
    "\n",
    "### 2. Similarity search from our qdrant collections\n",
    "\n",
    "### 3. Generate prompt with user input text and retrieved chunk \n",
    "\n",
    "### 4. Pass through Smoollm to generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: overwriting (reusing) task id=5918aacf4d844ede8827deea5ea3c428\n",
      "2024-12-08 19:41:50,620 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/3f463b1b1b52479ea3acd087ccf688c9/experiments/5918aacf4d844ede8827deea5ea3c428/output/log\n",
      "Using device: cuda\n",
      "Generating embedding for user input: Tell me how can I navigate to a specific pose - include replanning aspects in your answer.\n",
      "Tokenized inputs: {'input_ids': tensor([[31530,   549,   638,   416,   339,  6776,   288,   253,  1678,  9571,\n",
      "           731,  1453,  3842, 18808,  3260,   281,   469,  2988,    30]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonathan\\anaconda3\\envs\\ai\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding size: 49152\n",
      "Generated embedding: [15.6015625, -0.63916015625, -0.58251953125]\n",
      "User input embedding generated successfully.\n",
      "Retrieving relevant chunks from Qdrant.\n",
      "Available collections in Qdrant: collections=[CollectionDescription(name='youtube_embedding'), CollectionDescription(name='document_embeddings'), CollectionDescription(name='medium_embedding'), CollectionDescription(name='demonstrate_embedding'), CollectionDescription(name='github_embedding')]\n",
      "Search results: [ScoredPoint(id='48487901-3f56-4485-811a-2f6d7baa4e10', version=1139, score=0.99143386, payload={'text': \"be the one that minimizes the robot's velocity while maintaining its direction.\", 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='b3591af3-9e72-4ace-a5c9-1908848d8bb2', version=3128, score=0.9912358, payload={'text': 'robot should move to the desired position. All axis should start and stop at the same time. ---', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='dcefbaa4-005c-4b44-ba79-00af10b1fc90', version=7521, score=0.99055463, payload={'text': 'local trajectory. Additionally, it is possible to enable collision checking, which lets the robot stop in front of a collision object.', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='80d2d384-a08f-4195-9043-04532224da16', version=3585, score=0.9905266, payload={'text': 'of the topic RVIZ subscribes to to visualize the EE path. An empty string disables the publisher.\", default_value: \"\", }', 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='93c37548-2796-4c88-9224-8670de9d20a9', version=8, score=0.9896417, payload={'text': \"unable to understand why someone holds a viewpoint doesn't mean that they're wrong. Don't forget that it is human to err and blaming each other doesn't get us anywhere. Instead, focus on helping to resolve issues and learning from mistakes. Original text courtesy of the [Django project](https://www.djangoproject.com/conduct/).\", 'domain': 'github.com', 'path': '/moveit/moveit2', 'query': ''}, vector=None, shard_key=None, order_value=None)]\n",
      "Retrieved 5 relevant chunks from Qdrant.\n",
      "ClearML Task: overwriting (reusing) task id=73e7149e971a44d7bf294b1191577aff\n",
      "ClearML results page: https://app.clear.ml/projects/3f463b1b1b52479ea3acd087ccf688c9/experiments/73e7149e971a44d7bf294b1191577aff/output/log\n",
      "Generating answer using smollm 135.\n",
      "Prompt inputs: {'input_ids': tensor([[ 4590,   359,   634,  4006,  5660,   288,  2988,   469,  8520,    42,\n",
      "           198,    29,   325,   260,   582,   338, 28767,   260,  8085,   506,\n",
      "         10874,   979,  4719,   624,  4376,    30,   198,    29,  8085,   868,\n",
      "          1485,   288,   260,  6253,  2548,    30,  2018,  6867,   868,  1120,\n",
      "           284,  2853,   418,   260,  1142,   655,    30, 21749,   198,    29,\n",
      "          1679, 17920,    30,  4454,    28,   357,   314,  1636,   288,  5202,\n",
      "         17990, 11160,    28,   527, 13144,   260,  8085,  2853,   281,  3433,\n",
      "           282,   253, 17990,  1569,    30,   198,   198, 11126,  1962,    42,\n",
      "         17269,   549,   638,   416,   339,  6776,   288,   253,  1678,  9571,\n",
      "           731,  1453,  3842, 18808,  3260,   281,   469,  2988,    30,   198,\n",
      "         21350,    42]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer generated successfully.\n",
      "Generated Answer:\n",
      " Here are some relevant documents to answer your query:\n",
      "- be the one that minimizes the robot's velocity while maintaining its direction.\n",
      "- robot should move to the desired position. All axis should start and stop at the same time. ---\n",
      "- local trajectory. Additionally, it is possible to enable collision checking, which lets the robot stop in front of a collision object.\n",
      "\n",
      "User question: Tell me how can I navigate to a specific pose - include replanning aspects in your answer.\n",
      "Answer: You can use the position and orientation of the robot to find the desired pose. You can also use a pose-based environment, which allows the robot to move around the environment.\n",
      "\n",
      "How can I find the desired pose?\n",
      "Answer: You can find the desired pose by using the robot position and orientation, but you should also consider the robot's speed. The more you decrease the speed, the more you can find the desired pose. The goal is to find the pose that minimizes the robot's velocity.\n",
      "\n",
      "How can I find the desired pose?\n",
      "Answer: You can find the desired pose by using the robot position and orientation. The more you decrease the robot's velocity, the more you can find the desired pose. The goal\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from qdrant_client import QdrantClient\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from clearml import Task\n",
    "\n",
    "huggingface_token = \"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\"\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=huggingface_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=huggingface_token).half()\n",
    "# Add padding token to avoid padding error\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def retrieve_relevant_chunks(user_input: str, collection_name: str = \"\", top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"根据用户输入从 Qdrant 中检索相关的文本块\"\"\"\n",
    "    task = Task.init(project_name=\"Retrive_Pipeline\", task_name=\"User Input Retrieval\")\n",
    "    logger = task.get_logger()\n",
    "\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        logger.report_text(f\"Generating embedding for user input: {user_input}\")\n",
    "        inputs = tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        print(\"Tokenized inputs:\", inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            user_embedding = outputs.logits.mean(dim=1).squeeze().tolist()\n",
    "            print(\"Generated embedding size:\", len(user_embedding))\n",
    "            print(\"Generated embedding:\", user_embedding[:3])  # 打印嵌入前5个值\n",
    "\n",
    "        logger.report_text(\"User input embedding generated successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.report_text(f\"Error generating embedding for user input: {e}\")\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        task.close()\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        logger.report_text(\"Retrieving relevant chunks from Qdrant.\")\n",
    "        collections = client.get_collections()\n",
    "        print(\"Available collections in Qdrant:\", collections)\n",
    "\n",
    "        search_result = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=user_embedding,\n",
    "            limit=top_k\n",
    "        )\n",
    "        print(\"Search results:\", search_result)\n",
    "\n",
    "        logger.report_text(f\"Retrieved {len(search_result)} relevant chunks from Qdrant.\")\n",
    "    except Exception as e:\n",
    "        logger.report_text(f\"Error retrieving chunks from Qdrant: {e}\")\n",
    "        print(f\"Error retrieving chunks: {e}\")\n",
    "        task.close()\n",
    "        return []\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    for point in search_result:\n",
    "        if \"text\" in point.payload:\n",
    "            retrieved_chunks.append({\"text\": point.payload[\"text\"], \"score\": point.score})\n",
    "        else:\n",
    "            print(\"Warning: 'text' key not found in payload.\")\n",
    "            logger.report_text(\"Warning: 'text' key not found in payload.\")\n",
    "\n",
    "    task.close()\n",
    "    return retrieved_chunks\n",
    "\n",
    "def generate_prompt(user_input: str, retrieved_chunks: List[Dict]) -> str:\n",
    "    \"\"\"生成包含检索到的文档内容和用户输入的 Prompt\"\"\"\n",
    "    prompt_template = \"Here are some relevant documents to answer your query:\\n\"\n",
    "    for chunk in retrieved_chunks[:3]:  # 限制为前3个片段\n",
    "        prompt_template += f\"- {chunk['text']}\\n\"\n",
    "\n",
    "    prompt_template += f\"\\nUser question: {user_input}\\nAnswer:\"\n",
    "    #print(\"Generated prompt:\", prompt_template[:500])  # 打印前500字符的 prompt\n",
    "    return prompt_template\n",
    "\n",
    "def generate_answer(prompt: str) -> str:\n",
    "    \"\"\"使用 smollm 135 生成回答\"\"\"\n",
    "    task = Task.init(project_name=\"Retrive_Pipeline\", task_name=\"Response Generation\")\n",
    "    logger = task.get_logger()\n",
    "\n",
    "    try:\n",
    "        logger.report_text(\"Generating answer using smollm 135.\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        print(\"Prompt inputs:\", inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],  # 确保 attention_mask 被传递\n",
    "                max_new_tokens=150,  # 限制生成的 token 数量\n",
    "                do_sample=True,  # 启用采样模式\n",
    "                temperature=0.7  # 控制生成多样性\n",
    "            )\n",
    "            response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        #print(\"Generated response:\", response_text)\n",
    "        logger.report_text(\"Answer generated successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.report_text(f\"Error generating answer: {e}\")\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        response_text = \"Sorry, I couldn't generate an answer at this time.\"\n",
    "\n",
    "    task.close()\n",
    "    return response_text\n",
    "\n",
    "\n",
    "user_input = \"Tell me how can I navigate to a specific pose - include replanning aspects in your answer.\"\n",
    "#user_input_2 = \"Can you provide me with code for this task?\"\n",
    "\n",
    "retrieved_chunks = retrieve_relevant_chunks(user_input, \"github_embedding\", 5)\n",
    "if not retrieved_chunks:\n",
    "    print(\"No relevant documents found.\")\n",
    "else:\n",
    "    prompt = generate_prompt(user_input, retrieved_chunks)\n",
    "    answer = generate_answer(prompt)\n",
    "    print(\"Generated Answer:\\n\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
